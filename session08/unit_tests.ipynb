{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "unit-tests",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1r7LYI2OXxUT"
      },
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D_vXthkXxUW"
      },
      "source": [
        "## Unit Tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIbp40gXXxUX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "47f87ae5-1c9b-4455-dba7-389c3a75a847"
      },
      "source": [
        "nums = [1, 2, 4]\n",
        "\n",
        "assert sum(nums) == 6, \"Should be 6\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-1e1741302eed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnums\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnums\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Should be 6\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m: Should be 6"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPMpNGDCXxUY"
      },
      "source": [
        "Python includes the **unittest** module which provides testing automatization, namely to write preparing and finishing code snippets for all tests, grouping tests, etc.\n",
        "\n",
        "The basic concepts of the **unittest**:\n",
        "\n",
        "**Test fixture** - the preparation is done to run the tests along with any necessary cleanup after the tests. This can include, for example, creating temporary databases or starting a server process.\n",
        "\n",
        "**Test case** - minimal testing block. Checks answers on a given dataset. The unittest module provides the **TestCase** class which can be used to create new test cases.\n",
        "\n",
        "**Test suite** - A group of test cases or a group of several test groups. It's used to combine various tests into one pipeline.\n",
        "\n",
        "**Test runner** - a component that controls test execution and shows the results. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhmNmItwXxUY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "outputId": "d584d20e-f5a4-4630-ab68-f03b47e3da6e"
      },
      "source": [
        "#Example of testing the string methods\n",
        "\n",
        "import unittest\n",
        "\n",
        "class TestStringMethods(unittest.TestCase):\n",
        "    # test method names should start with test\n",
        "    def test_upper(self):\n",
        "        self.assertEqual('foo'.upper(), 'FOO')\n",
        "\n",
        "    def test_isupper(self):\n",
        "        self.assertTrue('FOO'.isupper())\n",
        "        self.assertFalse('Foo'.isupper())\n",
        "\n",
        "    def test_split(self):\n",
        "        s = 'hello world'\n",
        "        self.assertEqual(s.split(), ['hello', 'world'])\n",
        "        # Verify that s.split won't work for something that is not a string\n",
        "        with self.assertRaises(TypeError):\n",
        "            s.split(2)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    unittest.main()\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E\n",
            "======================================================================\n",
            "ERROR: /root/ (unittest.loader._FailedTest)\n",
            "----------------------------------------------------------------------\n",
            "AttributeError: module '__main__' has no attribute '/root/'\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.003s\n",
            "\n",
            "FAILED (errors=1)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQhB6JkDXxUZ"
      },
      "source": [
        "# Let's save this\n",
        "\n",
        "def dump_to(path):\n",
        "    with open(path, 'w') as f:\n",
        "        f.write(_i)  # _i is the \"last executed Input\" in iPython\n",
        "        \n",
        "dump_to('strings.py')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcsdAxDfXxUZ"
      },
      "source": [
        "## Oops.. Let's make unittest friends with jupyter notebook:\n",
        "\n",
        "Q: But what happened?\n",
        "\n",
        "A: The reason is that unittest.main looks at sys.argv and first parameter is what started IPython or Jupyter, therefore the error about kernel connection file not being a valid attribute. Passing explicit list to unittest.main will prevent IPython and Jupyter look at sys.argv. Passing exit=False will prevent unittest.main to shutdown the kernell process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGAR-4H7XxUa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ce37430-da73-4254-fea2-21b171dc0ab3"
      },
      "source": [
        "#Example of testing the string methods\n",
        "\n",
        "class TestStringMethods(unittest.TestCase):\n",
        "    # test method names should start with test\n",
        "    def test_upper(self):\n",
        "        self.assertEqual('foo'.upper(), 'FOO')\n",
        "\n",
        "    def test_isupper(self):\n",
        "        self.assertTrue('FOO'.isupper())\n",
        "        self.assertFalse('Foo'.isupper())\n",
        "\n",
        "    def test_split(self):\n",
        "        s = 'hello world'\n",
        "        self.assertEqual(s.split(), ['hello', 'world'])\n",
        "        # Verify that s.split won't work for something that is not a string\n",
        "        with self.assertRaises(TypeError):\n",
        "            s.split(2)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False)\n",
        "    "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "...\n",
            "----------------------------------------------------------------------\n",
            "Ran 3 tests in 0.010s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7fe4c89e5790>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zf95xlRpXxUb"
      },
      "source": [
        "## Command line interface"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xD2BWipWXxUb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc76040a-672f-4a8d-9358-e3ca815906fb"
      },
      "source": [
        "!python3 -m unittest strings                               # module\n",
        "!python3 -m unittest strings.TestStringMethods             # class\n",
        "!python3 -m unittest strings.TestStringMethods.test_split  # method"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "...\n",
            "----------------------------------------------------------------------\n",
            "Ran 3 tests in 0.000s\n",
            "\n",
            "OK\n",
            "...\n",
            "----------------------------------------------------------------------\n",
            "Ran 3 tests in 0.000s\n",
            "\n",
            "OK\n",
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.000s\n",
            "\n",
            "OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxqIXphXXxUc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e7a49b3-104e-4af0-fa5d-c704a3b6f24d"
      },
      "source": [
        "#The -v flag provides the more detailed report:\n",
        "!python3 -m unittest -v strings"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_isupper (strings.TestStringMethods) ... ok\n",
            "test_split (strings.TestStringMethods) ... ok\n",
            "test_upper (strings.TestStringMethods) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 3 tests in 0.000s\n",
            "\n",
            "OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FsuAnRvXxUc"
      },
      "source": [
        "## More flags:\n",
        "\n",
        "-b (--buffer) - the program output on test failure will be shown instead of hidden as usual.\n",
        "\n",
        "-c (--catch) - Ctrl + C, while a test is running, waits for the current test to complete and then reports the current results. Pressing Ctrl + C a second time throws a normal KeyboardInterrupt exception.\n",
        "\n",
        "-f (--failfast) - exit after the first failed test.\n",
        "\n",
        "--locals (starting with Python 3.5) - show local variables for failed tests."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1Yw9ybcXxUc"
      },
      "source": [
        "## Test detection\n",
        "\n",
        "unittest supports easy test detection. For compatibility with test detection, all test files must be modules or packages imported from the project's top-level directory.\n",
        "\n",
        "Test detection is implemented in TestLoader.discover (), but can be used from the command line:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2b05psuXxUc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83df96e5-03f0-4066-c679-9886ce58aa52"
      },
      "source": [
        "!mv strings.py test_strings.py  # rename the module to test....py to make it work\n",
        "!python3 -m  unittest  discover\n",
        "\n",
        "#-v (--verbose) - verbose output.\n",
        "#-s (--start-directory) directory_name - test detection start directory (current by default).\n",
        "#-p (--pattern) pattern - test file name template (test*.py by default).\n",
        "#-t (--top-level-directory) directory_name - project top-level directory (default is start-directory)."
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "...\n",
            "----------------------------------------------------------------------\n",
            "Ran 3 tests in 0.000s\n",
            "\n",
            "OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUtZVSdYXxUd"
      },
      "source": [
        "## Test code organization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5yXy-mYXxUd"
      },
      "source": [
        "# Create some class that will be tested\n",
        "\n",
        "class Widget():\n",
        "    \n",
        "    def __init__(self, name, x = 50, y = 50):\n",
        "        \n",
        "        self.name = name\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        \n",
        "    def size(self):\n",
        "        \n",
        "        return (self.x, self.y)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVeymagTXxUd"
      },
      "source": [
        "The basic test blocks are test cases - simple cases that must be checked for correctness.\n",
        "\n",
        "The test case is created by inheriting from unittest.TestCase.\n",
        "\n",
        "Testing code should be self-contained, that is, it should not depend in any way on other tests.\n",
        "\n",
        "The simplest TestCase subclass can simply implement a test method (the method starting with test).\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGFF9YDqXxUd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "917e7a2c-2d79-48bf-e65a-523d467ca467"
      },
      "source": [
        "\n",
        "class DefaultWidgetSizeTestCase(unittest.TestCase):\n",
        "    def test_default_widget_size(self):\n",
        "        widget = Widget('The widget')\n",
        "        self.assertEqual(widget.size(), (50, 50))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    unittest.main(argv=['',], defaultTest='DefaultWidgetSizeTestCase', exit=False)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.002s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7fe4c8a04710>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPFzScmHXxUe"
      },
      "source": [
        "There can be many tests, and some of the configuration code can be repeated. Fortunately, we can define the setup code by implementing a **setUp()** method that will run _before_ each test.\n",
        "\n",
        "We can also define a **tearDown()** method to run _after_ each test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fin1zUo8XxUe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16e45e2d-ebdd-4157-bb39-0343d44ce58d"
      },
      "source": [
        "\n",
        "class SimpleWidgetTestCase(unittest.TestCase):\n",
        "    def setUp(self):\n",
        "        self.widget = Widget('The widget')\n",
        "\n",
        "    def test_default_widget_size(self):\n",
        "        self.assertEqual(self.widget.size(), (50,50),\n",
        "                         'incorrect default size')\n",
        "\n",
        "    def test_widget_resize(self):\n",
        "        self.widget.resize(100,150)\n",
        "        self.assertEqual(self.widget.size(), (100,150),\n",
        "                         'wrong size after resize')\n",
        "        \n",
        "    def tearDown(self):\n",
        "        pass\n",
        "        \n",
        "if __name__ == '__main__':\n",
        "\n",
        "    unittest.main(argv=['',], defaultTest='SimpleWidgetTestCase', exit=False)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".E\n",
            "======================================================================\n",
            "ERROR: test_widget_resize (__main__.SimpleWidgetTestCase)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-11-a83988b03c0b>\", line 11, in test_widget_resize\n",
            "    self.widget.resize(100,150)\n",
            "AttributeError: 'Widget' object has no attribute 'resize'\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 2 tests in 0.006s\n",
            "\n",
            "FAILED (errors=1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7fe4c8a3d9d0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piUOHbXqXxUe"
      },
      "source": [
        "It is possible to place all tests in the same file as the program itself (such as widgets.py), but placing the tests in a separate file (such as test_widget.py) has many advantages:\n",
        "\n",
        "- The module with the test can be run autonomously from the command line.\n",
        "- Test code can be easily separated from the program.\n",
        "- Less temptation to change tests to match the program code for no apparent reason.\n",
        "- The test code should be changed much less frequently than the program.\n",
        "- Tested code can be more easily refactored.\n",
        "- Tests for C modules should be in separate modules, so why not be consistent?\n",
        "- If the testing strategy changes, there is no need to change the program code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQnwMuhPXxUe"
      },
      "source": [
        "## Test skipping and expected fails\n",
        "\n",
        "unittest supports skipping individual tests as well as test classes. In addition, it supports marking the test as \"not working, but it should be.\"\n",
        "\n",
        "The test is skipped using the **skip()** decorator or one of its conditional forms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZghrSbg2XxUe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f40e8f51-b440-4a87-f53d-692b272c636f"
      },
      "source": [
        "\n",
        "__version__ = (0, 9)\n",
        "platform = \"ubuntu\"\n",
        "\n",
        "\n",
        "class MyTestCase(unittest.TestCase):\n",
        "\n",
        "    @unittest.skip(\"demonstrating skipping\")\n",
        "    def test_nothing(self):\n",
        "        self.fail(\"shouldn't happen\")\n",
        "\n",
        "    @unittest.skipIf(__version__ < (1, 3),\n",
        "                     \"not supported in this library version\")\n",
        "    def test_format(self):\n",
        "        # Tests that work for only a certain version of the library.\n",
        "        pass\n",
        "\n",
        "    @unittest.skipUnless(platform.startswith(\"win\"), \"requires Windows\")\n",
        "    def test_windows_support(self):\n",
        "        # windows specific testing code\n",
        "        pass\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    unittest.main(argv=['','-v'], defaultTest='MyTestCase', exit=False)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_format (__main__.MyTestCase) ... skipped 'not supported in this library version'\n",
            "test_nothing (__main__.MyTestCase) ... skipped 'demonstrating skipping'\n",
            "test_windows_support (__main__.MyTestCase) ... skipped 'requires Windows'\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 3 tests in 0.008s\n",
            "\n",
            "OK (skipped=3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7fe4c8a0e590>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDr3cX0pXxUf"
      },
      "source": [
        "#### Classes may be skipped too:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMocWzr_XxUf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a8cefa0-778c-46e3-99fa-36f68d869bc2"
      },
      "source": [
        "@unittest.skip(\"showing class skipping\")\n",
        "class MySkippedTestCase(unittest.TestCase):\n",
        "    def test_not_run(self):\n",
        "        pass\n",
        "    \n",
        "if __name__ == '__main__':\n",
        "\n",
        "    unittest.main(argv=['','-v'], defaultTest='MySkippedTestCase', exit=False)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_not_run (__main__.MySkippedTestCase) ... skipped 'showing class skipping'\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.005s\n",
            "\n",
            "OK (skipped=1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7fe4c8a0e510>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1p0CxNlXxUf"
      },
      "source": [
        "#### expectedFailure() is used for expected fails:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ueH0CmsXxUf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4308cae3-4c9d-4455-8bb2-3117ba585f7e"
      },
      "source": [
        "class ExpectedFailureTestCase(unittest.TestCase):\n",
        "    @unittest.expectedFailure\n",
        "    def test_fail(self):\n",
        "        self.assertEqual(1, 0, \"broken\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    unittest.main(argv=['','-v'], defaultTest='ExpectedFailureTestCase', exit=False)        "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_fail (__main__.ExpectedFailureTestCase) ... expected failure\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.003s\n",
            "\n",
            "OK (expected failures=1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7fe4c8a13c90>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bLfwbqrXxUf"
      },
      "source": [
        "#### It's very easy to make your own decorator. For example, the following decorator skips the test if the passed object does not have the specified attribute.\n",
        "\n",
        "\n",
        "SetUp() and tearDown() are not triggered for missed tests. SetUpClass() and tearDownClass() are not triggered for missing classes. For missing modules setUpModule() and tearDownModule() are not triggered."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xn64A6Z3XxUf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17b3c50e-7626-4948-a70d-6cd855603f44"
      },
      "source": [
        "obj1 = [1, 2, 3]\n",
        "\n",
        "\n",
        "def skipUnlessHasattr(obj, attr):\n",
        "    if hasattr(obj, attr):\n",
        "        return lambda func: func\n",
        "    return unittest.skip(\"{!r} doesn't have {!r}\".format(obj, attr))\n",
        "\n",
        "\n",
        "class YetAnotherTestCase(unittest.TestCase):\n",
        "    @skipUnlessHasattr(obj1,'add')\n",
        "    def test_fail(self):\n",
        "        pass\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    unittest.main(argv=['','-v'], defaultTest='YetAnotherTestCase', exit=False)        "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_fail (__main__.YetAnotherTestCase) ... skipped \"[1, 2, 3] doesn't have 'add'\"\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.003s\n",
            "\n",
            "OK (skipped=1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7fe4c8371e10>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-B6hX2TCXxUf"
      },
      "source": [
        "#### Hey, what's up with setUpClass() and setUpModule() ??"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fP5VVi2XxUf"
      },
      "source": [
        "import unittest\n",
        "\n",
        "class Test(unittest.TestCase):\n",
        "    @classmethod\n",
        "    def setUpClass(cls):\n",
        "        cls._connection = createExpensiveConnectionObject()\n",
        "\n",
        "    @classmethod\n",
        "    def tearDownClass(cls):\n",
        "        cls._connection.destroy()\n",
        "        \n",
        "\n",
        "#These should be implemented as functions:\n",
        "\n",
        "def setUpModule():\n",
        "    createConnection()\n",
        "\n",
        "def tearDownModule():\n",
        "    closeConnection()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDNKMhJ4XxUf"
      },
      "source": [
        "#### Distinguishing test iterations using subtests\n",
        "\n",
        "When some tests have only minor differences, such as some parameters, unittest allows you to distinguish them within a single test method using the **subTest()** context manager."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0oXrewPXxUf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50846f67-4a3d-47da-bfc5-3e7712d2876a"
      },
      "source": [
        "class NumbersTest(unittest.TestCase):\n",
        "\n",
        "    def test_even(self):\n",
        "        \"\"\"\n",
        "        Test that numbers between 0 and 3 are all even.\n",
        "        \"\"\"\n",
        "        for i in range(0, 4):\n",
        "            with self.subTest(i=i):\n",
        "                self.assertEqual(i % 2, 0)\n",
        "                \n",
        "unittest.main(argv=['','-v'], defaultTest='NumbersTest', exit=False)           "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR\n",
            "\n",
            "======================================================================\n",
            "ERROR: setUpModule (__main__)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-16-6f21f4dc3d5c>\", line 16, in setUpModule\n",
            "    createConnection()\n",
            "NameError: name 'createConnection' is not defined\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 0 tests in 0.002s\n",
            "\n",
            "FAILED (errors=1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7fe4c89e5110>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mob457hxXxUg"
      },
      "source": [
        "#### Success checks\n",
        "\n",
        "The unittest module provides many functions for a wide variety of tests:\n",
        "\n",
        "```\n",
        "assertEqual(a, b) — a == b\n",
        "\n",
        "assertNotEqual(a, b) — a != b\n",
        "\n",
        "assertTrue(x) — bool(x) is True\n",
        "\n",
        "assertFalse(x) — bool(x) is False\n",
        "\n",
        "assertIs(a, b) — a is b\n",
        "\n",
        "assertIsNot(a, b) — a is not b\n",
        "\n",
        "assertIsNone(x) — x is None\n",
        "\n",
        "assertIsNotNone(x) — x is not None\n",
        "\n",
        "assertIn(a, b) — a in b\n",
        "\n",
        "assertNotIn(a, b) — a not in b\n",
        "\n",
        "assertIsInstance(a, b) — isinstance(a, b)\n",
        "\n",
        "assertNotIsInstance(a, b) — not isinstance(a, b)\n",
        "\n",
        "assertRaises(exc, fun, *args, **kwds) — fun(*args, **kwds) raises exc exception\n",
        "\n",
        "assertRaisesRegex(exc, r, fun, *args, **kwds) — fun(*args, **kwds) throws exc exception and message matches regex r\n",
        "\n",
        "assertWarns(warn, fun, *args, **kwds) — fun(*args, **kwds) raises warning\n",
        "\n",
        "assertWarnsRegex(warn, r, fun, *args, **kwds) — fun(*args, **kwds) raises warning and message matches regex r\n",
        "\n",
        "assertAlmostEqual(a, b) — round(a-b, 7) == 0\n",
        "\n",
        "assertNotAlmostEqual(a, b) — round(a-b, 7) != 0\n",
        "\n",
        "assertGreater(a, b) — a > b\n",
        "\n",
        "assertGreaterEqual(a, b) — a >= b\n",
        "\n",
        "assertLess(a, b) — a < b\n",
        "\n",
        "assertLessEqual(a, b) — a <= b\n",
        "\n",
        "assertRegex(s, r) — r.search(s)\n",
        "\n",
        "assertNotRegex(s, r) — not r.search(s)\n",
        "\n",
        "assertCountEqual(a, b) — a & b contain the same elements in the same quantities, but the order is not important\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pD_E54f9XxUg"
      },
      "source": [
        "#### To customize the execution of given tests:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbTsPPEKXxUg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb07dec7-e548-4287-9666-9c5154d58298"
      },
      "source": [
        "def MySuite():\n",
        "    suite = unittest.TestSuite()\n",
        "    suite.addTest(SimpleWidgetTestCase('test_default_widget_size'))\n",
        "    suite.addTest(SimpleWidgetTestCase('test_widget_resize'))\n",
        "    suite.addTest(NumbersTest('test_even'))\n",
        "    suite.addTest(YetAnotherTestCase('test_fail'))\n",
        "    return suite\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    runner = unittest.TextTestRunner(verbosity=2)\n",
        "    runner.run(MySuite())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR\n",
            "\n",
            "======================================================================\n",
            "ERROR: setUpModule (__main__)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-16-6f21f4dc3d5c>\", line 16, in setUpModule\n",
            "    createConnection()\n",
            "NameError: name 'createConnection' is not defined\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 0 tests in 0.001s\n",
            "\n",
            "FAILED (errors=1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=0 errors=1 failures=0>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}